{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification for Spoken Word Recognition\n",
    "\n",
    "## A Lightweight Classification Approach vs Generative Conformer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Identification\n",
    "\n",
    "### Initial Approach (Suboptimal)\n",
    "- A **Conformer-based generative model** was initially trained\n",
    "- Training loss reduced from **~400 â†’ ~1.25**, indicating convergence\n",
    "- Model size: **~2 million trainable parameters**\n",
    "\n",
    "### The Issue\n",
    "- The actual requirement is **NOT sequence generation**\n",
    "- The generative approach is over-engineered for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actual Task Definition\n",
    "\n",
    "| Attribute | Value |\n",
    "|-----------|-------|\n",
    "| **Task Type** | Closed-set spoken word classification |\n",
    "| **Input** | Short audio utterance |\n",
    "| **Output** | One label from a fixed vocabulary |\n",
    "| **Number of Classes** | 151+ predefined word classes |\n",
    "| **Vocabulary** | Closed (no open vocabulary) |\n",
    "| **Text Generation** | Not required |\n",
    "\n",
    "**This is a multi-class classification problem, NOT ASR.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why the Generative Conformer is Suboptimal\n",
    "\n",
    "### Designed For:\n",
    "- Sequence-to-sequence learning\n",
    "- Temporal alignment\n",
    "- Language modeling\n",
    "\n",
    "### Requires:\n",
    "- Large datasets (~70,000+ samples)\n",
    "- Higher compute resources\n",
    "- Longer inference latency\n",
    "\n",
    "### Problems:\n",
    "- Over-parameterized for a 151-class task\n",
    "- Unnecessary complexity\n",
    "- Higher deployment cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Proposed Solution: Direct Classification Model\n",
    "\n",
    "Replace generative modeling with **direct classification** using a lightweight architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchaudio librosa scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cpu\n",
      "Torchaudio version: 2.10.0+cpu\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchaudio version: {torchaudio.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Patch: Robust audio loading with clear error for missing FFmpeg/torchcodec\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load audio file with fallback methods and clear error if FFmpeg/torchcodec missing.\"\"\"\n",
    "    file_path = str(file_path)\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(file_path, backend='soundfile')\n",
    "        return waveform, sr\n",
    "    except Exception as e1:\n",
    "        pass\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(file_path, backend='sox')\n",
    "        return waveform, sr\n",
    "    except Exception as e2:\n",
    "        pass\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        return waveform, sr\n",
    "    except Exception as e3:\n",
    "        pass\n",
    "    try:\n",
    "        from scipy.io import wavfile as scipy_wavfile\n",
    "        sr, data = scipy_wavfile.read(file_path)\n",
    "        if data.dtype == np.int16:\n",
    "            data = data.astype(np.float32) / 32768.0\n",
    "        elif data.dtype == np.int32:\n",
    "            data = data.astype(np.float32) / 2147483648.0\n",
    "        elif data.dtype == np.uint8:\n",
    "            data = (data.astype(np.float32) - 128) / 128.0\n",
    "        else:\n",
    "            data = data.astype(np.float32)\n",
    "        if len(data.shape) == 1:\n",
    "            waveform = torch.from_numpy(data).unsqueeze(0)\n",
    "        else:\n",
    "            waveform = torch.from_numpy(data.T)\n",
    "        return waveform, sr\n",
    "    except Exception as e4:\n",
    "        pass\n",
    "    raise RuntimeError(\n",
    "        f\"Could not load audio file: {file_path}\\n\"\n",
    "        \"This may be due to missing or incompatible FFmpeg or torchcodec libraries required by torchaudio.\\n\"\n",
    "        \"To fix: Install FFmpeg and torchcodec, and ensure they are available in your environment.\\n\"\n",
    "        \"Try running: !pip install torch torchaudio torchcodec ffmpeg-python\\n\"\n",
    "        \"If on Windows, download FFmpeg from https://ffmpeg.org/download.html and add it to your PATH.\\n\"\n",
    "        f\"Original errors: {e1}, {e2}, {e3}, {e4 if 'e4' in locals() else ''}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">\n",
    "> If you encounter errors related to missing FFmpeg or torchcodec libraries (e.g., `Could not load libtorchcodec`), please ensure:\n",
    "> - FFmpeg is installed and available in your system PATH.\n",
    "> - The Python packages `torch`, `torchaudio`, and `torchcodec` are installed and compatible.\n",
    "> - You can install them with:\n",
    ">   ```python\n",
    ">   !pip install torch torchaudio torchcodec ffmpeg-python\n",
    ">   ```\n",
    "> - On Windows, download FFmpeg from https://ffmpeg.org/download.html and add it to your PATH.\n",
    "> - Restart the kernel after installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_classified\n",
      "Directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(r\"C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_classified\")\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_DURATION = 3.0  # seconds\n",
    "N_MFCC = 40\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 160  # 10ms at 16kHz\n",
    "WIN_LENGTH = 400  # 25ms at 16kHz\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Directory exists: {DATA_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1420\n",
      "Number of classes: 186\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>class_name</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_1769769626512.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_1769770595944.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_1769770837767.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_1769770999603.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_1769771391074.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_1769771759512.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_1769773788276.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_Burr_Material_Printed.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_Coming_off_Not_inserting.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>Basket_Crack_Tear.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path class_name  \\\n",
       "0  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "1  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "2  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "3  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "4  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "5  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "6  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "7  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "8  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "9  C:\\Users\\Bramha.nimbalkar\\Desktop\\LG-2\\audio_c...     Basket   \n",
       "\n",
       "                              filename  \n",
       "0             Basket_1769769626512.wav  \n",
       "1             Basket_1769770595944.wav  \n",
       "2             Basket_1769770837767.wav  \n",
       "3             Basket_1769770999603.wav  \n",
       "4             Basket_1769771391074.wav  \n",
       "5             Basket_1769771759512.wav  \n",
       "6             Basket_1769773788276.wav  \n",
       "7     Basket_Burr_Material_Printed.wav  \n",
       "8  Basket_Coming_off_Not_inserting.wav  \n",
       "9                Basket_Crack_Tear.wav  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all audio files and their labels\n",
    "def load_dataset_info(data_dir):\n",
    "    \"\"\"Scan the classified audio directory and create a dataset.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for class_folder in sorted(data_dir.iterdir()):\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name\n",
    "            for audio_file in class_folder.glob(\"*.wav\"):\n",
    "                data.append({\n",
    "                    'file_path': str(audio_file),\n",
    "                    'class_name': class_name,\n",
    "                    'filename': audio_file.name\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = load_dataset_info(DATA_DIR)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Number of classes: {df['class_name'].nunique()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "Min samples per class: 1\n",
      "Max samples per class: 64\n",
      "Mean samples per class: 7.63\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Class distribution\n",
    "class_counts = df['class_name'].value_counts()\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Min samples per class: {class_counts.min()}\")\n",
    "print(f\"Max samples per class: {class_counts.max()}\")\n",
    "print(f\"Mean samples per class: {class_counts.mean():.2f}\")\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(range(len(class_counts)), class_counts.values)\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 186\n",
      "\n",
      "First 20 classes: ['BU', 'Basket', 'Bending', 'Bottom', 'Bulging', 'Burn', 'Burr', 'Button', 'C', 'C-Fan', 'CF', 'CO', 'CS', 'Cap', 'Cap_DecoF', 'Cap_DecoR', 'Case', 'Case_LampCover', 'Choking', 'Clogging']\n"
     ]
    }
   ],
   "source": [
    "# Create label mapping\n",
    "classes = sorted(df['class_name'].unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "idx_to_class = {idx: cls for cls, idx in class_to_idx.items()}\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nFirst 20 classes: {classes[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction\n",
    "\n",
    "### 6.1 Audio Features\n",
    "- **MFCC** (Mel-Frequency Cepstral Coefficients)\n",
    "- **Log-Mel Spectrogram**\n",
    "- Frame length: 25 ms\n",
    "- Hop length: 10 ms\n",
    "- Feature dimension: 40-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor initialized\n"
     ]
    }
   ],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    \"\"\"Extract audio features for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000, n_mfcc=40, n_mels=80, \n",
    "                 hop_length=160, win_length=400, max_len=300):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_mels = n_mels\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.max_len = max_len  # Max frames\n",
    "        \n",
    "        # MFCC transform\n",
    "        self.mfcc_transform = T.MFCC(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "                'n_fft': 512,\n",
    "                'n_mels': n_mels,\n",
    "                'hop_length': hop_length,\n",
    "                'win_length': win_length,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Mel spectrogram transform\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=512,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            n_mels=n_mels,\n",
    "        )\n",
    "        \n",
    "    def extract_mfcc(self, waveform):\n",
    "        \"\"\"Extract MFCC features.\"\"\"\n",
    "        mfcc = self.mfcc_transform(waveform)\n",
    "        return self._pad_or_truncate(mfcc)\n",
    "    \n",
    "    def extract_mel_spectrogram(self, waveform):\n",
    "        \"\"\"Extract log-mel spectrogram.\"\"\"\n",
    "        mel = self.mel_transform(waveform)\n",
    "        log_mel = torch.log(mel + 1e-9)\n",
    "        return self._pad_or_truncate(log_mel)\n",
    "    \n",
    "    def _pad_or_truncate(self, features):\n",
    "        \"\"\"Pad or truncate features to fixed length.\"\"\"\n",
    "        # features shape: (1, n_features, time)\n",
    "        time_dim = features.shape[-1]\n",
    "        \n",
    "        if time_dim > self.max_len:\n",
    "            features = features[..., :self.max_len]\n",
    "        elif time_dim < self.max_len:\n",
    "            padding = self.max_len - time_dim\n",
    "            features = F.pad(features, (0, padding))\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Test feature extractor\n",
    "feature_extractor = AudioFeatureExtractor()\n",
    "print(\"Feature extractor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchcodec in c:\\users\\bramha.nimbalkar\\appdata\\local\\miniconda3\\lib\\site-packages (0.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchcodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Augmentation\n",
    "\n",
    "To improve model robustness:\n",
    "- **Noise injection**\n",
    "- **Time masking** (SpecAugment)\n",
    "- **Frequency masking** (SpecAugment)\n",
    "- **Time stretching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation module initialized\n"
     ]
    }
   ],
   "source": [
    "class AudioAugmentation:\n",
    "    \"\"\"Audio augmentation techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # SpecAugment transforms\n",
    "        self.time_mask = T.TimeMasking(time_mask_param=30)\n",
    "        self.freq_mask = T.FrequencyMasking(freq_mask_param=15)\n",
    "        \n",
    "    def add_noise(self, waveform, noise_level=0.005):\n",
    "        \"\"\"Add Gaussian noise.\"\"\"\n",
    "        noise = torch.randn_like(waveform) * noise_level\n",
    "        return waveform + noise\n",
    "    \n",
    "    def time_shift(self, waveform, shift_max=0.2):\n",
    "        \"\"\"Random time shift.\"\"\"\n",
    "        shift = int(np.random.uniform(-shift_max, shift_max) * waveform.shape[-1])\n",
    "        return torch.roll(waveform, shift, dims=-1)\n",
    "    \n",
    "    def spec_augment(self, spectrogram):\n",
    "        \"\"\"Apply SpecAugment (time and frequency masking).\"\"\"\n",
    "        augmented = self.time_mask(spectrogram)\n",
    "        augmented = self.freq_mask(augmented)\n",
    "        return augmented\n",
    "    \n",
    "    def apply_random(self, waveform, p=0.5):\n",
    "        \"\"\"Apply random augmentation with probability p.\"\"\"\n",
    "        if np.random.random() < p:\n",
    "            waveform = self.add_noise(waveform)\n",
    "        if np.random.random() < p:\n",
    "            waveform = self.time_shift(waveform)\n",
    "        return waveform\n",
    "\n",
    "augmenter = AudioAugmentation()\n",
    "print(\"Augmentation module initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class AudioClassificationDataset(Dataset):\n",
    "    \"\"\"Dataset for audio classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, file_paths, labels, class_to_idx, \n",
    "                 sample_rate=16000, max_len=300, \n",
    "                 feature_type='mel', augment=False):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_len = max_len\n",
    "        self.feature_type = feature_type\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.feature_extractor = AudioFeatureExtractor(\n",
    "            sample_rate=sample_rate, max_len=max_len\n",
    "        )\n",
    "        self.augmenter = AudioAugmentation(sample_rate=sample_rate)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "        \n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = T.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Apply augmentation (only during training)\n",
    "        if self.augment:\n",
    "            waveform = self.augmenter.apply_random(waveform, p=0.5)\n",
    "        \n",
    "        # Extract features\n",
    "        if self.feature_type == 'mfcc':\n",
    "            features = self.feature_extractor.extract_mfcc(waveform)\n",
    "        else:  # mel spectrogram\n",
    "            features = self.feature_extractor.extract_mel_spectrogram(waveform)\n",
    "        \n",
    "        # Apply SpecAugment during training\n",
    "        if self.augment and np.random.random() < 0.5:\n",
    "            features = self.augmenter.spec_augment(features)\n",
    "        \n",
    "        return features.squeeze(0), label\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 1420\n",
      "Filtered samples (classes with >=3 samples): 1408\n",
      "Original classes: 186\n",
      "Valid classes: 179\n",
      "\n",
      "Number of classes for training: 179\n",
      "\n",
      "Training samples: 985\n",
      "Validation samples: 208\n",
      "Test samples: 209\n"
     ]
    }
   ],
   "source": [
    "# Split dataset - Handle classes with few samples\n",
    "# Filter out classes with less than 3 samples (needed for stratified split)\n",
    "class_counts = df['class_name'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 3].index.tolist()\n",
    "df_filtered = df[df['class_name'].isin(valid_classes)].copy()\n",
    "\n",
    "print(f\"Original samples: {len(df)}\")\n",
    "print(f\"Filtered samples (classes with >=3 samples): {len(df_filtered)}\")\n",
    "print(f\"Original classes: {df['class_name'].nunique()}\")\n",
    "print(f\"Valid classes: {len(valid_classes)}\")\n",
    "\n",
    "# Update class mappings with filtered data\n",
    "classes = sorted(df_filtered['class_name'].unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "idx_to_class = {idx: cls for cls, idx in class_to_idx.items()}\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(f\"\\nNumber of classes for training: {NUM_CLASSES}\")\n",
    "\n",
    "# Perform stratified split\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_filtered, test_size=0.3, \n",
    "    stratify=df_filtered['class_name'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Filter temp_df to remove classes with <2 samples before next split\n",
    "min_samples = 2\n",
    "temp_class_counts = temp_df['class_name'].value_counts()\n",
    "valid_temp_classes = temp_class_counts[temp_class_counts >= min_samples].index.tolist()\n",
    "temp_df = temp_df[temp_df['class_name'].isin(valid_temp_classes)].copy()\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, \n",
    "    stratify=temp_df['class_name'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "BATCH_SIZE = 32\n",
    "MAX_FRAMES = 300\n",
    "FEATURE_TYPE = 'mel'  # 'mel' or 'mfcc'\n",
    "\n",
    "train_dataset = AudioClassificationDataset(\n",
    "    file_paths=train_df['file_path'].tolist(),\n",
    "    labels=train_df['class_name'].tolist(),\n",
    "    class_to_idx=class_to_idx,\n",
    "    max_len=MAX_FRAMES,\n",
    "    feature_type=FEATURE_TYPE,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = AudioClassificationDataset(\n",
    "    file_paths=val_df['file_path'].tolist(),\n",
    "    labels=val_df['class_name'].tolist(),\n",
    "    class_to_idx=class_to_idx,\n",
    "    max_len=MAX_FRAMES,\n",
    "    feature_type=FEATURE_TYPE,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = AudioClassificationDataset(\n",
    "    file_paths=test_df['file_path'].tolist(),\n",
    "    labels=test_df['class_name'].tolist(),\n",
    "    class_to_idx=class_to_idx,\n",
    "    max_len=MAX_FRAMES,\n",
    "    feature_type=FEATURE_TYPE,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test data loading\n",
    "# sample_batch, sample_labels = next(iter(train_loader))\n",
    "# print(f\"Batch shape: {sample_batch.shape}\")\n",
    "# print(f\"Labels shape: {sample_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Architectures\n",
    "\n",
    "### Proposed Lightweight Models:\n",
    "1. **CNN + TDNN** (Time-Delay Neural Network)\n",
    "2. **CRNN** (CNN + BiLSTM)\n",
    "3. **Mini-Conformer Encoder** (no decoder)\n",
    "4. **Depthwise Separable CNN**\n",
    "\n",
    "**Target:** â‰¤ 500k parameters, low-latency inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier - Trainable parameters: 319,987\n"
     ]
    }
   ],
   "source": [
    "# Model 1: CNN-based Classifier\n",
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"Lightweight CNN for audio classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_mels=80, num_classes=151, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, n_mels, time)\n",
    "        x = x.unsqueeze(1)  # Add channel dim: (batch, 1, n_mels, time)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test CNN model\n",
    "cnn_model = CNNClassifier(n_mels=N_MELS, num_classes=NUM_CLASSES)\n",
    "cnn_params = sum(p.numel() for p in cnn_model.parameters() if p.requires_grad)\n",
    "print(f\"CNN Classifier - Trainable parameters: {cnn_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN Classifier - Trainable parameters: 2,060,532\n"
     ]
    }
   ],
   "source": [
    "# Model 2: CRNN (CNN + BiLSTM)\n",
    "class CRNNClassifier(nn.Module):\n",
    "    \"\"\"CRNN for audio classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_mels=80, num_classes=151, hidden_size=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN feature extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "        )\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        self.cnn_out_size = 128 * (n_mels // 8)\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.cnn_out_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, n_mels, time)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.unsqueeze(1)  # (batch, 1, n_mels, time)\n",
    "        x = self.cnn(x)  # (batch, 128, n_mels//8, time)\n",
    "        \n",
    "        # Reshape for LSTM: (batch, time, features)\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, time, 128, n_mels//8)\n",
    "        x = x.reshape(batch_size, x.size(1), -1)  # (batch, time, 128 * n_mels//8)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, time, hidden*2)\n",
    "        \n",
    "        # Attention pooling\n",
    "        attn_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(context)\n",
    "        return output\n",
    "\n",
    "# Test CRNN model\n",
    "crnn_model = CRNNClassifier(n_mels=N_MELS, num_classes=NUM_CLASSES)\n",
    "crnn_params = sum(p.numel() for p in crnn_model.parameters() if p.requires_grad)\n",
    "print(f\"CRNN Classifier - Trainable parameters: {crnn_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightweight Classifier - Trainable parameters: 176,819\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Depthwise Separable CNN (MobileNet-style)\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"Depthwise separable convolution block.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size, stride, padding, groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class LightweightClassifier(nn.Module):\n",
    "    \"\"\"Lightweight classifier using depthwise separable convolutions.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_mels=80, num_classes=151, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.ds_blocks = nn.Sequential(\n",
    "            DepthwiseSeparableConv(32, 64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            DepthwiseSeparableConv(64, 128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            DepthwiseSeparableConv(128, 128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            DepthwiseSeparableConv(128, 256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.ds_blocks(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test Lightweight model\n",
    "light_model = LightweightClassifier(n_mels=N_MELS, num_classes=NUM_CLASSES)\n",
    "light_params = sum(p.numel() for p in light_model.parameters() if p.requires_grad)\n",
    "print(f\"Lightweight Classifier - Trainable parameters: {light_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-Conformer Classifier - Trainable parameters: 1,191,188\n"
     ]
    }
   ],
   "source": [
    "# Model 4: Mini-Conformer Encoder (Classification only, no decoder)\n",
    "class ConformerBlock(nn.Module):\n",
    "    \"\"\"Simplified Conformer block for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=128, n_heads=4, ff_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feed-forward module 1\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Convolution module\n",
    "        self.conv_norm = nn.LayerNorm(d_model)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model * 2, kernel_size=1),\n",
    "            nn.GLU(dim=1),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=31, padding=15, groups=d_model),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Feed-forward module 2\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, d_model)\n",
    "        \n",
    "        # FF1 with residual (scale 0.5)\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "        \n",
    "        # Self-attention with residual\n",
    "        attn_in = self.attn_norm(x)\n",
    "        attn_out, _ = self.attn(attn_in, attn_in, attn_in)\n",
    "        x = x + self.attn_dropout(attn_out)\n",
    "        \n",
    "        # Convolution with residual\n",
    "        conv_in = self.conv_norm(x)\n",
    "        conv_in = conv_in.transpose(1, 2)  # (batch, d_model, time)\n",
    "        conv_out = self.conv(conv_in)\n",
    "        conv_out = conv_out.transpose(1, 2)  # (batch, time, d_model)\n",
    "        x = x + conv_out\n",
    "        \n",
    "        # FF2 with residual (scale 0.5)\n",
    "        x = x + 0.5 * self.ff2(x)\n",
    "        \n",
    "        x = self.final_norm(x)\n",
    "        return x\n",
    "\n",
    "class MiniConformerClassifier(nn.Module):\n",
    "    \"\"\"Mini Conformer for audio classification (encoder only).\"\"\"\n",
    "    \n",
    "    def __init__(self, n_mels=80, num_classes=151, d_model=128, n_layers=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Subsampling (reduce time dimension)\n",
    "        self.subsample = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Linear projection\n",
    "        subsample_out = 32 * (n_mels // 4)\n",
    "        self.proj = nn.Linear(subsample_out, d_model)\n",
    "        \n",
    "        # Conformer blocks\n",
    "        self.conformer_blocks = nn.ModuleList([\n",
    "            ConformerBlock(d_model=d_model, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head with attention pooling\n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, n_mels, time)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Subsampling\n",
    "        x = x.unsqueeze(1)  # (batch, 1, n_mels, time)\n",
    "        x = self.subsample(x)  # (batch, 32, n_mels//4, time//4)\n",
    "        \n",
    "        # Reshape: (batch, time, features)\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, time//4, 32, n_mels//4)\n",
    "        x = x.reshape(batch_size, x.size(1), -1)  # (batch, time//4, 32*n_mels//4)\n",
    "        \n",
    "        # Project to d_model\n",
    "        x = self.proj(x)  # (batch, time//4, d_model)\n",
    "        \n",
    "        # Conformer blocks\n",
    "        for block in self.conformer_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Attention pooling\n",
    "        attn_weights = F.softmax(self.attention_pool(x), dim=1)\n",
    "        x = torch.sum(attn_weights * x, dim=1)  # (batch, d_model)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n",
    "# Test Mini-Conformer\n",
    "conformer_model = MiniConformerClassifier(n_mels=N_MELS, num_classes=NUM_CLASSES)\n",
    "conformer_params = sum(p.numel() for p in conformer_model.parameters() if p.requires_grad)\n",
    "print(f\"Mini-Conformer Classifier - Trainable parameters: {conformer_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      "Model                               Parameters\n",
      "------------------------------------------------------------\n",
      "CNN Classifier                         319,987\n",
      "CRNN Classifier                      2,060,532\n",
      "Lightweight (DS-Conv)                  176,819\n",
      "Mini-Conformer                       1,191,188\n",
      "------------------------------------------------------------\n",
      "Original Conformer (ref)            ~2,000,000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Model comparison summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<30} {'Parameters':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'CNN Classifier':<30} {cnn_params:>15,}\")\n",
    "print(f\"{'CRNN Classifier':<30} {crnn_params:>15,}\")\n",
    "print(f\"{'Lightweight (DS-Conv)':<30} {light_params:>15,}\")\n",
    "print(f\"{'Mini-Conformer':<30} {conformer_params:>15,}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Original Conformer (ref)':<30} {'~2,000,000':>15}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Setup\n",
    "\n",
    "- **Loss:** Categorical Cross-Entropy\n",
    "- **Optimizer:** AdamW\n",
    "- **Learning Rate Scheduler:** OneCycleLR\n",
    "- **Batch Size:** 32-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for features, labels in pbar:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(val_loader, desc='Validating'):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=1e-3, device='cuda'):\n",
    "    \"\"\"Full training loop.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # OneCycleLR scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"âœ“ Saved best model with val_acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return history, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: lightweight\n",
      "Parameters: 176,819\n"
     ]
    }
   ],
   "source": [
    "# Select model for training\n",
    "# Options: CNNClassifier, CRNNClassifier, LightweightClassifier, MiniConformerClassifier\n",
    "\n",
    "MODEL_CHOICE = 'lightweight'  # 'cnn', 'crnn', 'lightweight', 'conformer'\n",
    "\n",
    "if MODEL_CHOICE == 'cnn':\n",
    "    model = CNNClassifier(n_mels=N_MELS, num_classes=NUM_CLASSES)\n",
    "elif MODEL_CHOICE == 'crnn':\n",
    "    model = CRNNClassifier(n_mels=N_MELS, num_classes=NUM_CLASSES)\n",
    "elif MODEL_CHOICE == 'lightweight':\n",
    "    model = LightweightClassifier(n_mels=N_MELS, num_classes=NUM_CLASSES)\n",
    "elif MODEL_CHOICE == 'conformer':\n",
    "    model = MiniConformerClassifier(n_mels=N_MELS, num_classes=NUM_CLASSES)\n",
    "\n",
    "print(f\"Selected model: {MODEL_CHOICE}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  - Epochs: 50\n",
      "  - Learning rate: 0.001\n",
      "  - Batch size: 32\n",
      "  - Device: cpu\n",
      "  - Feature type: mel\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(f\"  - Feature type: {FEATURE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/31 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8, and we attempt to load libtorchcodec\n             for each of those versions. Errors for versions not installed on\n             your system are expected; only the error for your installed FFmpeg\n             version is relevant. On Windows, ensure you've installed the\n             \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.10.0+cpu) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\n\nFFmpeg version 7:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\n\nFFmpeg version 6:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\n\nFFmpeg version 5:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\n\nFFmpeg version 4:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history, best_acc = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed! Best validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, lr, device)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     35\u001b[39m val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, scheduler, device)\u001b[39m\n\u001b[32m      6\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mAudioClassificationDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     25\u001b[39m label = \u001b[38;5;28mself\u001b[39m.class_to_idx[\u001b[38;5;28mself\u001b[39m.labels[idx]]\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Load audio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m waveform, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Resample if necessary\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sr != \u001b[38;5;28mself\u001b[39m.sample_rate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchaudio\\__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchaudio\\_torchcodec.py:82\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Import torchcodec here to provide clear error if not available\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTorchCodec is required for load_with_torchcodec. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mPlease install torchcodec to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\__init__.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Note: usort wants to put Frame and FrameBatch after decoders and samplers,\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# but that results in circular import.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSamples, Frame, FrameBatch  \u001b[38;5;66;03m# usort:skip # noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decoders, encoders, samplers, transforms  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Note that version.py is generated during install.\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\decoders\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioStreamMetadata, VideoStreamMetadata\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_audio_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decoder_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_cuda_backend  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     AudioStreamMetadata,\n\u001b[32m     10\u001b[39m     ContainerMetadata,\n\u001b[32m     11\u001b[39m     get_container_metadata,\n\u001b[32m     12\u001b[39m     get_container_metadata_from_header,\n\u001b[32m     13\u001b[39m     VideoStreamMetadata,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     _add_video_stream,\n\u001b[32m     17\u001b[39m     _get_backend_details,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     seek_to_pts,\n\u001b[32m     46\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\_metadata.py:16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfractions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Fraction\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _get_container_json_metadata,\n\u001b[32m     18\u001b[39m     _get_stream_json_metadata,\n\u001b[32m     19\u001b[39m     create_from_file,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     23\u001b[39m SPACES = \u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mStreamMetadata\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py:109\u001b[39m\n\u001b[32m    105\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m os.add_dll_directory(\u001b[38;5;28mstr\u001b[39m(ffmpeg_dir))  \u001b[38;5;66;03m# that's the actual CM\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m expose_ffmpeg_dlls():\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     ffmpeg_major_version, core_library_path = \u001b[43mload_torchcodec_shared_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Note: We use disallow_in_graph because PyTorch does constant propagation of\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# factory functions.\u001b[39;00m\n\u001b[32m    114\u001b[39m create_from_file = torch._dynamo.disallow_in_graph(\n\u001b[32m    115\u001b[39m     torch.ops.torchcodec_ns.create_from_file.default\n\u001b[32m    116\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py:76\u001b[39m, in \u001b[36mload_torchcodec_shared_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     69\u001b[39m         exceptions.append((ffmpeg_major_version, exc_traceback))\n\u001b[32m     71\u001b[39m traceback_info = (\n\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, tb \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[32m     74\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[33m[end of libtorchcodec loading traceback].\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     77\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m      1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m         versions 4, 5, 6, 7, and 8, and we attempt to load libtorchcodec\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[33m         for each of those versions. Errors for versions not installed on\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[33m         your system are expected; only the error for your installed FFmpeg\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[33m         version is relevant. On Windows, ensure you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve installed the\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33m         \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfull-shared\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m version which ships DLLs.\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[33m      2. The PyTorch version (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is not compatible with\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[33m         this version of TorchCodec. Refer to the version compatibility\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[33m         table:\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[33m         https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[33m      3. Another runtime dependency; see exceptions below.\u001b[39m\n\u001b[32m     89\u001b[39m \n\u001b[32m     90\u001b[39m \u001b[33m    The following exceptions were raised as we tried to load libtorchcodec:\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8, and we attempt to load libtorchcodec\n             for each of those versions. Errors for versions not installed on\n             your system are expected; only the error for your installed FFmpeg\n             version is relevant. On Windows, ensure you've installed the\n             \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.10.0+cpu) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\n\nFFmpeg version 7:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\n\nFFmpeg version 6:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\n\nFFmpeg version 5:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\n\nFFmpeg version 4:\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1442, in load_library\n    ctypes.CDLL(path)\n    ~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\ctypes\\__init__.py\", line 390, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ~~~~~~~^^^^^^^^^^^^^^^^^^\nFileNotFoundError: Could not find module 'C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 57, in load_torchcodec_shared_libraries\n    torch.ops.load_library(core_library_path)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\_ops.py\", line 1444, in load_library\n    raise OSError(f\"Could not load this library: {path}\") from e\nOSError: Could not load this library: C:\\Users\\Bramha.nimbalkar\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback]."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history, best_acc = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m14\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Loss plot\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m axes[\u001b[32m0\u001b[39m].plot(\u001b[43mhistory\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTrain Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m axes[\u001b[32m0\u001b[39m].plot(history[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mVal Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m axes[\u001b[32m0\u001b[39m].set_xlabel(\u001b[33m'\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHsAAAGyCAYAAAB0jsg1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIQZJREFUeJzt3X2MFtXZB+CzgICmgloKCEWpWkWLgoJsQY2xoZJosPzRlKoBSvyo1RoLaQVEwW+srxqSukpErf5RC2rEGCFYpRJjpSGCJNoKRlGhRhao5aOooDBvZprdsrhYF3efnb33upIRZnZmn7PPkZ17f3vmnKosy7IEAAAAQAgdWrsBAAAAADQfYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAAEIiwBwAAACAQYQ8AAABAew57XnrppTR69OjUp0+fVFVVlZ5++un/ec3SpUvTaaedlrp06ZKOO+649MgjjxxoewEA2hS1EwBQ+rBnx44dadCgQammpuYrnf/uu++m888/P51zzjlp1apV6Ve/+lW69NJL03PPPXcg7QUAaFPUTgBApVVlWZYd8MVVVWnBggVpzJgx+z1nypQpaeHChemNN96oP/bTn/40bdmyJS1evPhAXxoAoM1ROwEAldCppV9g2bJlaeTIkQ2OjRo1qhjhsz87d+4stjp79uxJH330UfrmN79ZFEkAQDnlv0Pavn178bh3hw6mBqxU7ZRTPwFA25S1QP3U4mHPhg0bUq9evRocy/e3bduWPvnkk3TwwQd/4ZpZs2alm266qaWbBgC0kPXr16dvf/vb3t8K1U459RMAtG3rm7F+avGw50BMmzYtTZ48uX5/69at6aijjiq+8G7durVq2wCA/csDiX79+qVDDz3U21Rh6icAaJu2tUD91OJhT+/evVNtbW2DY/l+Htrs7zdT+apd+bav/BphDwCUn8euK1s75dRPANC2VTXjtDUt/jD98OHD05IlSxoce/7554vjAAConQCA5tXksOff//53sYR6vtUtrZ7/fd26dfVDiMePH19//hVXXJHWrl2brr322rR69ep03333pccffzxNmjSpOb8OAIBSUjsBAKUPe1599dV06qmnFlsun1sn//uMGTOK/Q8//LA++Ml95zvfKZZez0fzDBo0KN19993pwQcfLFaVAACITu0EAFRaVZav8dUGJivq3r17MVGzOXsAoLzcs8tDXwBA+71nt/icPQAAAABUjrAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAoL2HPTU1Nal///6pa9euqbq6Oi1fvvxLz589e3Y64YQT0sEHH5z69euXJk2alD799NMDbTMAQJujfgIAShv2zJ8/P02ePDnNnDkzrVy5Mg0aNCiNGjUqbdy4sdHzH3vssTR16tTi/DfffDM99NBDxee47rrrmqP9AAClp34CAEod9txzzz3psssuSxMnTkwnnXRSmjNnTjrkkEPSww8/3Oj5r7zySjrjjDPSRRddVIwGOvfcc9OFF174P0cDAQBEoX4CAEob9uzatSutWLEijRw58r+foEOHYn/ZsmWNXjNixIjimrpwZ+3atWnRokXpvPPO2+/r7Ny5M23btq3BBgDQFqmfAIBK69SUkzdv3px2796devXq1eB4vr969epGr8lH9OTXnXnmmSnLsvT555+nK6644ksf45o1a1a66aabmtI0AIBSUj8BAOFW41q6dGm6/fbb03333VfM8fPUU0+lhQsXpltuuWW/10ybNi1t3bq1flu/fn1LNxMAoDTUTwBAxUb29OjRI3Xs2DHV1tY2OJ7v9+7du9FrbrjhhjRu3Lh06aWXFvsnn3xy2rFjR7r88svT9OnTi8fA9tWlS5diAwBo69RPAECpR/Z07tw5DRkyJC1ZsqT+2J49e4r94cOHN3rNxx9//IVAJw+McvljXQAAkamfAIBSj+zJ5cuuT5gwIQ0dOjQNGzYszZ49uxipk6/OlRs/fnzq27dvMe9ObvTo0cUKFKeeemqqrq5Ob7/9djHaJz9eF/oAAESmfgIASh32jB07Nm3atCnNmDEjbdiwIQ0ePDgtXry4ftLmdevWNRjJc/3116eqqqrizw8++CB961vfKoKe2267rXm/EgCAklI/AQCVVJW1gWep8qXXu3fvXkzW3K1bt9ZuDgCwH+7Z5aEvAKD93rNbfDUuAAAAACpH2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAAAQiLAHAAAAIBBhDwAAAEAgwh4AAACAQIQ9AAAAAIEIewAAAAACEfYAAAAABCLsAQAAAAhE2AMAAADQ3sOempqa1L9//9S1a9dUXV2dli9f/qXnb9myJV111VXpyCOPTF26dEnHH398WrRo0YG2GQCgzVE/AQCV0qmpF8yfPz9Nnjw5zZkzpwh6Zs+enUaNGpXWrFmTevbs+YXzd+3alX74wx8WH3vyySdT37590/vvv58OO+yw5voaAABKTf0EAFRSVZZlWVMuyAOe008/Pd17773F/p49e1K/fv3S1VdfnaZOnfqF8/NQ6P/+7//S6tWr00EHHXRAjdy2bVvq3r172rp1a+rWrdsBfQ4AoOW5ZzdO/QQAVLJ+atJjXPkonRUrVqSRI0f+9xN06FDsL1u2rNFrnnnmmTR8+PDiMa5evXqlgQMHpttvvz3t3r17v6+zc+fO4ovdewMAaIvUTwBApTUp7Nm8eXMR0uShzd7y/Q0bNjR6zdq1a4vHt/Lr8nl6brjhhnT33XenW2+9db+vM2vWrCLVqtvykUMAAG2R+gkACLcaV/6YVz5fzwMPPJCGDBmSxo4dm6ZPn1483rU/06ZNK4Yv1W3r169v6WYCAJSG+gkAqNgEzT169EgdO3ZMtbW1DY7n+7179270mnwFrnyunvy6OieeeGIxEigf1ty5c+cvXJOv2JVvAABtnfoJACj1yJ48mMlH5yxZsqTBb57y/XxensacccYZ6e233y7Oq/PWW28VIVBjQQ8AQCTqJwCg9I9x5cuuz507Nz366KPpzTffTL/4xS/Sjh070sSJE4uPjx8/vngMq07+8Y8++ihdc801RcizcOHCYoLmfMJmAID2QP0EAJT2Ma5cPufOpk2b0owZM4pHsQYPHpwWL15cP2nzunXrihW66uSTKz/33HNp0qRJ6ZRTTkl9+/Ytgp8pU6Y071cCAFBS6icAoJKqsizLUjtccx4AaH7u2eWhLwCg/d6zW3w1LgAAAAAqR9gDAAAAEIiwBwAAACAQYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAAEIiwBwAAACAQYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAAEIiwBwAAACAQYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAAEIiwBwAAACAQYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAAEIiwBwAAACAQYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAAEIiwBwAAACAQYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAAEIiwBwAAACAQYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAAEIiwBwAAACAQYQ8AAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAhH2AAAAAAQi7AEAAAAIRNgDAAAA0N7DnpqamtS/f//UtWvXVF1dnZYvX/6Vrps3b16qqqpKY8aMOZCXBQBos9RPAEBpw5758+enyZMnp5kzZ6aVK1emQYMGpVGjRqWNGzd+6XXvvfde+vWvf53OOuusr9NeAIA2R/0EAJQ67LnnnnvSZZddliZOnJhOOumkNGfOnHTIIYekhx9+eL/X7N69O1188cXppptuSsccc8zXbTMAQJuifgIAShv27Nq1K61YsSKNHDnyv5+gQ4dif9myZfu97uabb049e/ZMl1xyyVd6nZ07d6Zt27Y12AAA2iL1EwBQ6rBn8+bNxSidXr16NTie72/YsKHRa15++eX00EMPpblz537l15k1a1bq3r17/davX7+mNBMAoDTUTwBAqNW4tm/fnsaNG1cEPT169PjK102bNi1t3bq1flu/fn1LNhMAoDTUTwDA19WpKSfngU3Hjh1TbW1tg+P5fu/evb9w/jvvvFNMzDx69Oj6Y3v27PnPC3fqlNasWZOOPfbYL1zXpUuXYgMAaOvUTwBAqUf2dO7cOQ0ZMiQtWbKkQXiT7w8fPvwL5w8YMCC9/vrradWqVfXbBRdckM4555zi7x7PAgCiUz8BAKUe2ZPLl12fMGFCGjp0aBo2bFiaPXt22rFjR7E6V278+PGpb9++xbw7Xbt2TQMHDmxw/WGHHVb8ue9xAICo1E8AQKnDnrFjx6ZNmzalGTNmFJMyDx48OC1evLh+0uZ169YVK3QBAKB+AgAqryrLsiyVXL70er4qVz5Zc7du3Vq7OQDAfrhnl4e+AID2e882BAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQHsPe2pqalL//v1T165dU3V1dVq+fPl+z507d24666yz0uGHH15sI0eO/NLzAQAiUj8BAKUNe+bPn58mT56cZs6cmVauXJkGDRqURo0alTZu3Njo+UuXLk0XXnhhevHFF9OyZctSv3790rnnnps++OCD5mg/AEDpqZ8AgEqqyrIsa8oF+Uie008/Pd17773F/p49e4oA5+qrr05Tp079n9fv3r27GOGTXz9+/Piv9Jrbtm1L3bt3T1u3bk3dunVrSnMBgApyz26c+gkAqGT91KSRPbt27UorVqwoHsWq/wQdOhT7+aidr+Ljjz9On332WTriiCP2e87OnTuLL3bvDQCgLVI/AQCV1qSwZ/PmzcXInF69ejU4nu9v2LDhK32OKVOmpD59+jQIjPY1a9asItWq2/KRQwAAbZH6CQAIvRrXHXfckebNm5cWLFhQTO68P9OmTSuGL9Vt69evr2QzAQBKQ/0EADRVp6ac3KNHj9SxY8dUW1vb4Hi+37t37y+99q677iqKlRdeeCGdcsopX3puly5dig0AoK1TPwEApR7Z07lz5zRkyJC0ZMmS+mP5BM35/vDhw/d73Z133pluueWWtHjx4jR06NCv12IAgDZE/QQAlHpkTy5fdn3ChAlFaDNs2LA0e/bstGPHjjRx4sTi4/kKW3379i3m3cn99re/TTNmzEiPPfZY6t+/f/3cPt/4xjeKDQAgOvUTAFDqsGfs2LFp06ZNRYCTBzeDBw8uRuzUTdq8bt26YoWuOvfff3+xCsWPf/zjBp9n5syZ6cYbb2yOrwEAoNTUTwBAJVVlWZaldrjmPADQ/Nyzy0NfAED7vWdXdDUuAAAAAFqWsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAACgvYc9NTU1qX///qlr166puro6LV++/EvPf+KJJ9KAAQOK808++eS0aNGiA20vAECbpH4CAEob9syfPz9Nnjw5zZw5M61cuTINGjQojRo1Km3cuLHR81955ZV04YUXpksuuSS99tpracyYMcX2xhtvNEf7AQBKT/0EAFRSVZZlWVMuyEfynH766enee+8t9vfs2ZP69euXrr766jR16tQvnD927Ni0Y8eO9Oyzz9Yf+/73v58GDx6c5syZ85Vec9u2bal79+5p69atqVu3bk1pLgBQQe7ZjVM/AQCVrJ86NeXkXbt2pRUrVqRp06bVH+vQoUMaOXJkWrZsWaPX5MfzkUB7y0cCPf300/t9nZ07dxZbnfwLrnsDAIDyqrtXN/F3SaGpnwCAStdPTQp7Nm/enHbv3p169erV4Hi+v3r16kav2bBhQ6Pn58f3Z9asWemmm276wvF8BBEAUH7//Oc/i99QoX4CACpfPzUp7KmUfOTQ3qOBtmzZko4++ui0bt06hWMrp4154LZ+/XqP07UyfVEe+qIc9EN55KNxjzrqqHTEEUe0dlPaHfVTOfn+VB76ohz0Q3noi9j1U5PCnh49eqSOHTum2traBsfz/d69ezd6TX68KefnunTpUmz7yhMuc/a0vrwP9EM56Ivy0BfloB/KI3/Mm/9QP5Hz/ak89EU56Ify0Bcx66cmfabOnTunIUOGpCVLltQfyydozveHDx/e6DX58b3Pzz3//PP7PR8AIBL1EwBQaU1+jCt/vGrChAlp6NChadiwYWn27NnFalsTJ04sPj5+/PjUt2/fYt6d3DXXXJPOPvvsdPfdd6fzzz8/zZs3L7366qvpgQceaP6vBgCghNRPAECpw558KfVNmzalGTNmFJMs50uoL168uH4S5nxenb2HHo0YMSI99thj6frrr0/XXXdd+u53v1usxDVw4MCv/Jr5I10zZ85s9NEuKkc/lIe+KA99UQ76oTz0RePUT+2XfxPloS/KQT+Uh76I3RdVmbVRAQAAAMIweyIAAABAIMIeAAAAgECEPQAAAACBCHsAAAAAAilN2FNTU5P69++funbtmqqrq9Py5cu/9PwnnngiDRgwoDj/5JNPTosWLapYWyNrSj/MnTs3nXXWWenwww8vtpEjR/7PfqNl+mJv8+bNS1VVVWnMmDHe7lbqiy1btqSrrroqHXnkkcWM+scff7zvUa3QD7Nnz04nnHBCOvjgg1O/fv3SpEmT0qefftocTWm3XnrppTR69OjUp0+f4vtMvrrm/7J06dJ02mmnFf8WjjvuuPTII49UpK3tgdqpPNRP5aF+Kge1U3mon9px/ZSVwLx587LOnTtnDz/8cPa3v/0tu+yyy7LDDjssq62tbfT8v/zlL1nHjh2zO++8M/v73/+eXX/99dlBBx2Uvf766xVveyRN7YeLLrooq6mpyV577bXszTffzH72s59l3bt3z/7xj39UvO3tvS/qvPvuu1nfvn2zs846K/vRj35UsfZG1tS+2LlzZzZ06NDsvPPOy15++eWiT5YuXZqtWrWq4m1vz/3whz/8IevSpUvxZ94Hzz33XHbkkUdmkyZNqnjbI1m0aFE2ffr07KmnnsryEmLBggVfev7atWuzQw45JJs8eXJxv/7d735X3L8XL15csTZHpXYqD/VTeaifykHtVB7qp/ZdP5Ui7Bk2bFh21VVX1e/v3r0769OnTzZr1qxGz//JT36SnX/++Q2OVVdXZz//+c9bvK2RNbUf9vX5559nhx56aPboo4+2YCvbhwPpi/z9HzFiRPbggw9mEyZMEPa0Ul/cf//92THHHJPt2rWruZrAAfRDfu4PfvCDBsfyG+YZZ5zh/WwmX6VYufbaa7Pvfe97DY6NHTs2GzVqlH74mtRO5aF+Kg/1UzmoncpD/dS+66dWf4xr165dacWKFcUjQHU6dOhQ7C9btqzRa/Lje5+fGzVq1H7Pp2X6YV8ff/xx+uyzz9IRRxzhLW+Fvrj55ptTz5490yWXXOL9b8W+eOaZZ9Lw4cOLx7h69eqVBg4cmG6//fa0e/du/VLBfhgxYkRxTd2jXmvXri0epTvvvPP0QwW5X7cMtVN5qJ/KQ/1UDmqn8lA/tV3NVT91Sq1s8+bNxQ9B+Q9Fe8v3V69e3eg1GzZsaPT8/DiV64d9TZkypXgOcd//MWn5vnj55ZfTQw89lFatWuXtbuW+yEOFP//5z+niiy8uwoW33347XXnllUUQOnPmTP1ToX646KKLiuvOPPPMfARr+vzzz9MVV1yRrrvuOn1QQfu7X2/bti198sknxXxKNJ3aqTzUT+WhfioHtVN5qJ/aruaqn1p9ZA8x3HHHHcXEwAsWLCgmT6Vytm/fnsaNG1dMmN2jRw9vfSvbs2dPMcLqgQceSEOGDEljx45N06dPT3PmzGntprUr+aR2+Yiq++67L61cuTI99dRTaeHChemWW25p7aYB1FM/tR71U3moncpD/RRLq4/syX847dixY6qtrW1wPN/v3bt3o9fkx5tyPi3TD3Xuuuuuolh54YUX0imnnOLtrnBfvPPOO+m9994rZnjf+6aZ69SpU1qzZk069thj9UsF+iKXr8B10EEHFdfVOfHEE4uEPh9O27lzZ31RgX644YYbihD00ksvLfbzVRt37NiRLr/88iJ8yx8Do+Xt737drVs3o3q+BrVTeaifykP9VA5qp/JQP7VdzVU/tXq1m//gk//2e8mSJQ1+UM3383kvGpMf3/v83PPPP7/f82mZfsjdeeedxW/KFy9enIYOHeqtboW+GDBgQHr99deLR7jqtgsuuCCdc845xd/zJaepTF/kzjjjjOLRrbrALffWW28VIZCgp3L9kM8htm+gUxfA/WduPCrB/bplqJ3KQ/1UHuqnclA7lYf6qe1qtvopK8mScPkSuY888kixtNjll19eLKm7YcOG4uPjxo3Lpk6d2mDp9U6dOmV33XVXseT3zJkzLb3eCv1wxx13FEshP/nkk9mHH35Yv23fvr05mtOuNbUv9mU1rtbri3Xr1hWr0v3yl7/M1qxZkz377LNZz549s1tvvbUZW9X+NLUf8vtC3g9//OMfi+Ur//SnP2XHHntssZojBy7//v7aa68VW15C3HPPPcXf33///eLjeR/kfbHv0qG/+c1vivt1TU2NpdebidqpPNRP5aF+Kge1U3mon9p3/VSKsCeXrx1/1FFHFeFBvkTcX//61/qPnX322cUPr3t7/PHHs+OPP744P1+WbOHCha3Q6nia0g9HH3108T/rvlv+QxaV7Yt9CXtaty9eeeWVrLq6uggn8mXYb7vttuzzzz9v5la1P03ph88++yy78cYbi4Cna9euWb9+/bIrr7wy+9e//tVKrY/hxRdfbPT7ft17n/+Z98W+1wwePLjot/zfw+9///tWan08aqfyUD+Vh/qpHNRO5aF+ar/1U1X+n+YddAQAAABAa2n1OXsAAAAAaD7CHgAAAIBAhD0AAAAAgQh7AAAAAAIR9gAAAAAEIuwBAAAACETYAwAAABCIsAcAAAAgEGEPAAAAQCDCHgAAAIBAhD0AAAAAgQh7AAAAAFIc/w/JLyUiKOJTlAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Acc')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_acc, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nClassification Report (Top 20 classes):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get unique labels in test set\n",
    "unique_labels = sorted(set(test_labels))\n",
    "target_names = [idx_to_class[i] for i in unique_labels]\n",
    "\n",
    "report = classification_report(\n",
    "    test_labels, test_preds,\n",
    "    labels=unique_labels[:20],\n",
    "    target_names=target_names[:20],\n",
    "    zero_division=0\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix (subset for visualization)\n",
    "n_display = 20  # Show top 20 classes\n",
    "\n",
    "# Get most common classes\n",
    "label_counts = Counter(test_labels)\n",
    "common_labels = [label for label, _ in label_counts.most_common(n_display)]\n",
    "\n",
    "# Filter predictions and labels for common classes\n",
    "mask = [l in common_labels for l in test_labels]\n",
    "filtered_labels = [l for l, m in zip(test_labels, mask) if m]\n",
    "filtered_preds = [p for p, m in zip(test_preds, mask) if m]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(filtered_labels, filtered_preds, labels=common_labels)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=[idx_to_class[i] for i in common_labels],\n",
    "    yticklabels=[idx_to_class[i] for i in common_labels]\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Top 20 Classes)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier:\n",
    "    \"\"\"Production-ready audio classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, class_mapping, device='cuda', \n",
    "                 sample_rate=16000, max_len=300):\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.idx_to_class = class_mapping\n",
    "        self.sample_rate = sample_rate\n",
    "        self.feature_extractor = AudioFeatureExtractor(\n",
    "            sample_rate=sample_rate, max_len=max_len\n",
    "        )\n",
    "        \n",
    "    def preprocess(self, audio_path):\n",
    "        \"\"\"Load and preprocess audio file.\"\"\"\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        if sr != self.sample_rate:\n",
    "            resampler = T.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        features = self.feature_extractor.extract_mel_spectrogram(waveform)\n",
    "        return features.squeeze(0)\n",
    "    \n",
    "    def predict(self, audio_path, top_k=5):\n",
    "        \"\"\"Predict class for audio file.\"\"\"\n",
    "        features = self.preprocess(audio_path)\n",
    "        features = features.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(features)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "        # Get top-k predictions\n",
    "        top_probs, top_indices = torch.topk(probs[0], top_k)\n",
    "        \n",
    "        predictions = []\n",
    "        for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):\n",
    "            predictions.append({\n",
    "                'class': self.idx_to_class[idx],\n",
    "                'probability': float(prob)\n",
    "            })\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_batch(self, audio_paths):\n",
    "        \"\"\"Batch prediction for multiple audio files.\"\"\"\n",
    "        results = []\n",
    "        for path in audio_paths:\n",
    "            pred = self.predict(path, top_k=1)[0]\n",
    "            results.append({\n",
    "                'file': path,\n",
    "                'prediction': pred['class'],\n",
    "                'confidence': pred['probability']\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = AudioClassifier(\n",
    "    model=model,\n",
    "    class_mapping=idx_to_class,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Audio classifier initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on sample files\n",
    "sample_files = test_df['file_path'].head(5).tolist()\n",
    "sample_labels = test_df['class_name'].head(5).tolist()\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for file_path, true_label in zip(sample_files, sample_labels):\n",
    "    predictions = classifier.predict(file_path, top_k=3)\n",
    "    \n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"\\nFile: {filename}\")\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(\"Predictions:\")\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {pred['class']}: {pred['probability']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Token-to-Code Mapping (Post-processing)\n",
    "\n",
    "Each classified word maps to a predefined system code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command mapping\n",
    "COMMAND_MAPPING = {\n",
    "    # Example mappings - customize based on your application\n",
    "    'Door': 'CMD_DOOR',\n",
    "    'Opening': 'CMD_OPEN',\n",
    "    'Closing': 'CMD_CLOSE',\n",
    "    'Cooling': 'CMD_COOL',\n",
    "    'Display': 'CMD_DISPLAY',\n",
    "    'Power': 'CMD_POWER',\n",
    "    'Button': 'CMD_BUTTON',\n",
    "    'Ice': 'CMD_ICE',\n",
    "    'Dispenser': 'CMD_DISPENSER',\n",
    "    'Basket': 'CMD_BASKET',\n",
    "    'Shelf': 'CMD_SHELF',\n",
    "    'Control': 'CMD_CONTROL',\n",
    "    'Noise': 'CMD_NOISE_ALERT',\n",
    "    'Leakage': 'CMD_LEAK_ALERT',\n",
    "    'Crack': 'CMD_DAMAGE_ALERT',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def word_to_command(word):\n",
    "    \"\"\"Map classified word to system command.\"\"\"\n",
    "    return COMMAND_MAPPING.get(word, f'CMD_{word.upper()}')\n",
    "\n",
    "def process_utterance(audio_path, classifier):\n",
    "    \"\"\"Process audio utterance and return command.\"\"\"\n",
    "    predictions = classifier.predict(audio_path, top_k=1)\n",
    "    predicted_word = predictions[0]['class']\n",
    "    confidence = predictions[0]['probability']\n",
    "    \n",
    "    command = word_to_command(predicted_word)\n",
    "    \n",
    "    return {\n",
    "        'word': predicted_word,\n",
    "        'command': command,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "# Test command mapping\n",
    "print(\"Command Mapping Examples:\")\n",
    "print(\"=\"*50)\n",
    "for word, cmd in list(COMMAND_MAPPING.items())[:10]:\n",
    "    print(f\"{word:20} â†’ {cmd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test full pipeline\n",
    "print(\"\\nFull Pipeline Test:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for file_path in sample_files[:3]:\n",
    "    result = process_utterance(file_path, classifier)\n",
    "    print(f\"\\nAudio: {os.path.basename(file_path)}\")\n",
    "    print(f\"  Recognized Word: {result['word']}\")\n",
    "    print(f\"  System Command: {result['command']}\")\n",
    "    print(f\"  Confidence: {result['confidence']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Export for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for deployment\n",
    "import json\n",
    "\n",
    "# Save model weights\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'n_mels': N_MELS,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'model_type': MODEL_CHOICE\n",
    "    }\n",
    "}, 'audio_classifier_deployment.pth')\n",
    "\n",
    "# Save class mapping\n",
    "with open('class_mapping.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'class_to_idx': class_to_idx,\n",
    "        'idx_to_class': {str(k): v for k, v in idx_to_class.items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Model saved for deployment!\")\n",
    "print(\"  - Model weights: audio_classifier_deployment.pth\")\n",
    "print(\"  - Class mapping: class_mapping.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX for edge deployment\n",
    "dummy_input = torch.randn(1, N_MELS, MAX_FRAMES).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    'audio_classifier.onnx',\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['audio_features'],\n",
    "    output_names=['class_logits'],\n",
    "    dynamic_axes={\n",
    "        'audio_features': {0: 'batch_size'},\n",
    "        'class_logits': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Model exported to ONNX format: audio_classifier.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Benefits Summary\n",
    "\n",
    "| Metric | Generative Conformer | Classification Model |\n",
    "|--------|---------------------|---------------------|\n",
    "| **Parameters** | ~2,000,000 | <500,000 |\n",
    "| **Training Data** | ~70,000 samples | 10,000-20,000 samples |\n",
    "| **Inference Latency** | High | Low |\n",
    "| **Convergence** | Slower | Faster |\n",
    "| **Deployment** | Complex | Simple (edge/embedded) |\n",
    "| **Task Fit** | Over-engineered | Optimal |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Conclusion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Architecture Mismatch**: The initial Conformer generative model was functionally correct but architecturally misaligned for the classification task.\n",
    "\n",
    "2. **Task Requirements**: The task requires **word-level classification**, not sequence generation.\n",
    "\n",
    "3. **Optimal Solution**: Switching to a lightweight classification model provides:\n",
    "   - âœ… Higher classification accuracy\n",
    "   - âœ… Lower latency\n",
    "   - âœ… Smaller model size (â‰¤500k params vs 2M)\n",
    "   - âœ… Faster convergence\n",
    "   - âœ… Lower data requirements\n",
    "   - âœ… Easier deployment on edge devices\n",
    "\n",
    "4. **Recommended Architecture**: The **Lightweight (Depthwise Separable CNN)** or **CNN Classifier** provides the best balance of accuracy and efficiency for this 151+ class classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
